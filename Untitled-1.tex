\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{booktabs}

\begin{document}

\title{Energy Efficient Underwater Waste Detection via Dynamic YOLO Model Switching Based on Real Time Image Quality Assessment}

\author{\IEEEauthorblockN{Sarthak Bhagwat, Amaan Ahmad}
\IEEEauthorblockA{School of Electronics and Communication Engineering\\
Vellore Institute of Technology, Chennai, India\\
Email: \{sarthak.bhagwat, amaan.ahmad\}2022@vitstudent.ac.in}
}

\maketitle

\begin{abstract}
Waste detection in autonomous underwater vehicles (AUVs) faces critical trade-offs between computational efficiency and detection accuracy. While high-capacity deep learning models achieve superior accuracy, their energy demands limit deployment in resource-constrained underwater missions. This paper presents a dynamic model switching framework that adaptively transitions between YOLOv8n (lightweight) and YOLOv8m (balanced) models based on real-time image quality metrics. The system evaluates blur, brightness, and contrast to compute a quality score triggering model transitions via hysteresis-based thresholds. Experimental results demonstrate that the proposed approach retains 86.4\% of YOLOv8m's detection accuracy while reducing average inference time by 52\% and extending operational autonomy by approximately 1.9×. The framework operates with the lightweight model 88.7\% of the time under normal conditions, switching to the balanced model only when image degradation necessitates enhanced processing. 
% Per-class analysis across 15 detection categories shows the adaptive system recovers 67.4\% of the performance gap between YOLOv8n and YOLOv8m. 
This adaptive mechanism effectively addresses the critical challenge of balancing detection performance with energy efficiency for long-duration underwater monitoring missions.  
\end{abstract}

\begin{IEEEkeywords}
Underwater waste detection, dynamic model switching, YOLOv8, image quality assessment, energy efficient deep learning, autonomous underwater vehicles, real-time object detection
\end{IEEEkeywords}

\section{Introduction}

Marine plastic pollution is one of the most urgent environmental issues of the 21st century, with an estimated 8--12 million tons entering the oceans each year on average \cite{Jambeck2015}. Around 70\% of this debris eventually sinks to the seafloor, forming deposits that are difficult to detect and remove using conventional methods \cite{Woodall2014}. Autonomous underwater vehicles (AUVs) equipped with onboard vision systems offer a more scalable solution for automated waste detection and monitoring, enabling systematic seabed mapping and targeted cleanup operations \cite{Fulton2005}.

Object detection using deep learning, especially the You Only Look Once (YOLO) family of models, has proven effective for waste detection in underwater environments \cite{Redmon2016}. However, fundamental constraints arise when deploying these models on resource-limited AUV platforms. High-accuracy models require substantial compute and power, whereas lightweight models suffer in adverse underwater conditions where turbidity, variable lighting, and optical distortions make detection significantly harder \cite{Akkaynak2019}. Existing underwater waste detection approaches typically employ a single, statically chosen model that is fixed for the entire mission, either targeting maximum accuracy or minimum latency \cite{Chen2020, Fulton2005}. Such static selection leads to suboptimal resource usage: high-capacity models may process clear, high-quality frames that are easy to detect, while lightweight models may be used in severely degraded scenes with low visibility or complex occlusions where their capacity is insufficient \cite{Li2020}. To the best of current knowledge, no deployed system dynamically adapts its inference strategy to ambient environmental conditions during operation.

This paper proposes an energy-conscious dual-model switching architecture that addresses these limitations through four main contributions. First, a computationally efficient image quality evaluation module is designed that measures blur, brightness, and contrast to produce a scalar quality score in less than 5~ms per frame. Second, a hysteresis-based switching mechanism with cooldown is introduced to avoid oscillation while enabling responsive transitions between YOLOv8n and YOLOv8m. Third, an analytics subsystem is developed to log frame-level metrics for 15 detection classes, including marine animals, ROV components, and multiple trash categories, to support post-mission analysis and operational tuning. Fourth, comprehensive empirical evaluation demonstrates a 52\% reduction in average inference time while retaining 86.4\% of the higher-capacity model's detection accuracy, with per-class analysis indicating consistent improvements across object categories.

The graphical user interface of the system, shown in Figure~\ref{fig:system_overview}, provides real-time performance monitoring and parameter adjustment for underwater waste detection. The interface allows operators to set quality thresholds, select model configurations, and visualize detection performance metrics during operation.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{UI.png}
\caption{System overview of the interface for real-time model switching and performance monitoring.}
\label{fig:system_overview}
\end{figure}

\section{Related Work}

Underwater imaging presents challenges distinct from terrestrial computer vision due to wavelength-dependent light absorption and scattering, which introduce color distortion, reduced contrast, and limited visibility \cite{Akkaynak2019, Schechner2005}. Suspended particles generate backscatter noise, and biological growth alters object appearance over time \cite{Jaffe2015}. Existing approaches attempt to mitigate these issues through physics-based image enhancement \cite{Drews2013, Peng2017}, though such methods introduce computational overhead and often generalize poorly across varying underwater conditions. End-to-end deep learning methods directly learn robust feature representations from degraded imagery \cite{Li2020, Fabbri2018}, but require substantial training data covering diverse environmental scenarios.

The YOLO family of detectors has evolved significantly since its introduction \cite{Redmon2016}, offering improved speed–accuracy trade-offs across multiple scaled variants. YOLOv8 provides five model sizes, ranging from lightweight (n) to high-capacity (x), each balancing inference cost and detection performance \cite{Jocher2023}. Prior works have demonstrated that YOLO-based models perform effectively in underwater object detection tasks \cite{Chen2020, Xu2021}. However, these implementations rely on static model selection, applying the same detector across all frames regardless of environmental variability.

Research on efficient deep learning has explored multiple strategies to reduce computation. Knowledge distillation transfers representations from large teacher networks to smaller student models \cite{Hinton2015}. Pruning removes redundant parameters to shrink network size \cite{Han2015}, while quantization reduces numerical precision to accelerate inference \cite{Jacob2018}. Early-exit architectures such as BranchyNet \cite{Teerapittayanon2016} and cascade detection pipelines \cite{Viola2001} offer adaptive computation based on confidence metrics, though they require architectural changes or specialized training procedures \cite{Liu2019}. These techniques improve efficiency but do not dynamically adjust computational capacity based on real-time image difficulty.

Specialized underwater waste detection systems have been developed, including early AUV vision pipelines \cite{Fulton2005} and recent deep-learning-based approaches leveraging the TrashCAN dataset \cite{Hong2020}. However, these systems do not incorporate dynamic inference strategies. The approach in this paper differs by maintaining two fully trained YOLO models and selecting between them based on real-time image quality assessment. This design avoids architectural modifications, requires no specialized training, and remains agnostic to the underlying detection models. The strategy enables controllable accuracy–efficiency trade-offs and generalizes to any pair of models with differing capacity characteristics.


\section{Methodology}

The dynamic model switching framework consists of four components operating jointly: an image quality assessment module, a hysteresis-based switching mechanism, a dual-model YOLO inference pipeline, and a performance analytics subsystem. Each component functions independently, allowing modular adaptation without architectural coupling.

\subsection{Image Quality Assessment}

The quality assessment module computes three normalized metrics—blur, brightness, and contrast—based on established no-reference image quality principles \cite{Mittal2012}. Blur estimation uses the variance of the Laplacian operator:

\begin{equation}
L = \nabla^2 I = \frac{\partial^2 I}{\partial x^2} + \frac{\partial^2 I}{\partial y^2}
\end{equation}
\begin{equation}
\sigma_L^2 = \text{var}(L)
\end{equation}

The sharpness measure is normalized through a sigmoid mapping:
\begin{equation}
S_{blur} = \frac{1}{1 + e^{-10(\sigma_L^2 / 500 - 0.3)}}
\end{equation}

Brightness assessment uses histogram statistics:
\begin{equation}
\mu_B = \frac{\sum_{i=0}^{255} i \cdot H(i)}{\sum_{i=0}^{255} H(i)}
\end{equation}

A range-based scoring function evaluates deviation from the optimal brightness interval:

\begin{equation}
S_{brightness} =
\begin{cases}
\min\!\left(1.0,\; 0.85 + 0.15\left(1 - 
\dfrac{|\mu_B - B_{center}|}{B_{range}/2}\right)\right), 
& B_{min} \leq \mu_B \leq B_{max} \\[6pt]

\max\!\left(0.1,\; \min\!\left(0.8,\; 0.8 - 0.7\, p_{sensitivity}\right)\right), 
& \text{otherwise}
\end{cases}
\end{equation}


Contrast is approximated using grayscale intensity standard deviation \cite{Peli1990}:
\begin{equation}
\sigma_I = \sqrt{\frac{1}{N}\sum_{i=1}^{N}(I_i - \mu_I)^2}
\end{equation}

A sigmoid normalizes contrast:
\begin{equation}
S_{contrast} = \frac{1}{1 + e^{-8(\sigma_I / 80 - 0.4)}}
\end{equation}

The final quality score is a weighted combination:
\begin{equation}
Q = 0.4 S_{blur} + 0.3 S_{brightness} + 0.3 S_{contrast}
\end{equation}

Blur receives the highest weight due to its dominant impact on feature extraction reliability in degraded underwater scenes \cite{Li2020}.


\subsection{Model Switching Strategy}

A hysteresis-based switching mechanism governs transitions between YOLOv8n and YOLOv8m to prevent oscillation during borderline quality conditions. Two thresholds, $\tau_{low}$ and $\tau_{high}$, define the hysteresis band, and a cooldown interval $\Delta f_{cooldown}$ enforces a minimum frame gap between consecutive switches.

\begin{algorithm}
\caption{Hysteresis-Based Model Switching}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Quality score $Q$, frame index $f$
\STATE \textbf{Parameters:} $\tau_{low}$, $\tau_{high}$, $\Delta f_{cooldown}$
\STATE \textbf{State:} $M_{current}$, $f_{last\_switch}$
\IF{$f - f_{last\_switch} < \Delta f_{cooldown}$}
    \STATE \textbf{return} $M_{current}$
\ENDIF
\IF{$M_{current} = n$ AND $Q < \tau_{low}$}
    \STATE $M_{current} \leftarrow m$
    \STATE $f_{last\_switch} \leftarrow f$
\ELSIF{$M_{current} = m$ AND $Q > \tau_{high}$}
    \STATE $M_{current} \leftarrow n$
    \STATE $f_{last\_switch} \leftarrow f$
\ENDIF
\STATE \textbf{return} $M_{current}$
\end{algorithmic}
\end{algorithm}

The system initializes with YOLOv8n to minimize startup computational overhead. Hysteresis prevents rapid toggling around boundary values, and cooldown provides temporal stability.

\subsection{Dataset and Model Training}

The TrashCAN 1.0 dataset \cite{Hong2020} is used for detector training and evaluation. It includes 7,212 annotated underwater frames across fifteen categories representing marine organisms, ROV components, and multiple waste types. Dataset splits maintain class balance across training (70\%), validation (15\%), and test sets (15\%).

YOLOv8n and YOLOv8m are fine-tuned from COCO pretrained weights using transfer learning \cite{Yosinski2014}. Both models are trained for 100 epochs with batch size 16 at $640 \times 640$ resolution using SGD (momentum = 0.937), cosine annealing, weight decay 0.0005, and augmentations including mosaic, mixup, flipping, and scale jitter \cite{Bochkovskiy2020}. YOLOv8n converges in 2.5 hours and YOLOv8m in 6.5 hours on an NVIDIA Tesla T4.

YOLOv8 employs a CSPNet backbone \cite{Wang2020}, C2f modules, SPPF \cite{He2015}, and a combined FPN–PAN neck \cite{Lin2017, Liu2018}. The detection head uses an anchor-free formulation and distribution-based bounding box regression.


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{lable.png}
\caption{Sample labeled underwater waste images from the TrashCAN 1.0 dataset showing multiple detection classes across diverse environmental conditions.}
\label{fig:dataset_samples}
\end{figure}

The dataset was split into training (70\%), validation (15\%), and test (15\%) subsets, maintaining class distribution balance across splits. The inclusion of ROV and biological life classes is critical for safe autonomous operation, preventing false positives on the AUV itself and avoiding harm to marine organisms \cite{Fulton2005}. Both YOLOv8 variants were trained from pretrained COCO weights using transfer learning \cite{Yosinski2014} on NVIDIA Tesla T4 (16GB VRAM) with CUDA 12.1. Training used 100 epochs with batch size 16, input resolution 640×640, SGD optimizer with momentum (0.937), initial learning rate 0.01 with cosine annealing schedule \cite{Loshchilov2017}, weight decay 0.0005, and data augmentation including mosaic, mixup, random flip, and scale jitter \cite{Bochkovskiy2020}. Training for YOLOv8n required approximately 2.5 hours, while YOLOv8m required 6.5 hours, with both models converging successfully after 80 epochs.

YOLOv8 uses a CSPNet-based backbone \cite{Wang2020} combined with C2f modules for every backbone feature extraction block. The backbone consists of an initial conv stem (3×3 convolution), four CSP bottleneck stages with progressive channel expansion, and SPPF (Spatial Pyramid Pooling Fast) to aggregate multi-scale features \cite{He2015}. The neck structure is composed of FPN (Feature Pyramid Network) for the top-down pathway \cite{Lin2017}, PAN (Path Aggregation Network) for the bottom-up pathway \cite{Liu2018}, both containing C2f modules at each fusion level for efficient feature mixing. The neck feeds into a decoupled head with one head each for classification and localization, an anchor-free design using distribution focal loss \cite{Li2020b}, and three detection scales (P3, P4, P5) for multi-scale prediction. The major difference between YOLOv8n and YOLOv8m has to do with the depth and width scaling factors applied to these components that were responsible for the difference in parameter count, 3.2M versus 25.9M parameters.

\subsection{Threshold Optimization}

Threshold selection is performed through grid evaluation across 20 $(\tau_{low}, \tau_{high})$ pairs. Each configuration is tested on multiple video sequences covering clear, moderate, turbid, low-light, and variable environments. Metrics include mAP, model usage ratio, switch count, and class-level performance.

The final recommended parameters for distinct underwater conditions are listed in Table~\ref{tab:env_params}, enabling environmental tuning without modifying core architecture.

\begin{table}[h]
\centering
\caption{Recommended Parameters for Different Underwater Conditions}
\label{tab:env_params}
\begin{tabular}{lcc}
\toprule
\textbf{Condition} & \textbf{$\tau_{low}$} & \textbf{$\tau_{high}$} \\
\midrule
Clear water, good light & 0.25 & 0.30 \\
Moderate turbidity & 0.32 & 0.36 \\
High turbidity & 0.40 & 0.45 \\
Low light conditions & 0.35 & 0.40 \\
Fast moving scenarios & 0.28 & 0.32 \\
\bottomrule
\end{tabular}
\end{table}

\section{Experimental Setup and Implementation}

System performance is evaluated using complementary accuracy and efficiency metrics. Detection accuracy is quantified using mAP@0.5, mAP@0.5:0.95, Precision ($P = \frac{TP}{TP + FP}$), Recall ($R = \frac{TP}{TP + FN}$), and the F1 score ($F1 = 2 \cdot \frac{P \cdot R}{P + R}$). Efficiency is measured through average FPS, mean inference latency, model usage distribution, switch frequency, and estimated energy savings relative to a YOLOv8m-only baseline.

Operational behaviour is further analysed using quality score distributions, switching latency, and correlations between quality assessments and detection performance. Two baselines are used for comparison: a static lightweight configuration (YOLOv8n only) and a static balanced-capacity configuration (YOLOv8m only).

Five underwater video sequences (1000 frames each) are used for evaluation, covering clear water, moderate turbidity, low-light deep water, high turbidity, and variable lighting conditions. Each sequence represents specific environmental challenges relevant to AUV mission profiles.

The system is implemented using Python 3.9.13, PyTorch 2.0.1, Ultralytics YOLOv8 8.0.196, OpenCV 4.8.0, NumPy 1.24.3, Matplotlib 3.7.1, and Pandas 2.0.3. Quality assessment employs blur sigmoid parameters $k = 10.0$ and $x_0 = 0.3$ with a variance cap of 500; brightness sensitivity of 1.0 with an optimal intensity range of 100--140; and contrast sigmoid parameters $k = 8.0$ and $x_0 = 0.4$ with a standard deviation cap of 80. Model switching uses thresholds $\tau_{low} = 0.32$, $\tau_{high} = 0.36$, a cooldown period of 10 frames, a frame check interval of 1, and initializes with YOLOv8n as the active model.

\section{Results and Analysis}

Baseline performance for each YOLOv8 variant is presented in Table~\ref{tab:individual_performance}. YOLOv8m achieves substantially higher detection accuracy, with an mAP@0.5 of 0.455 compared to 0.265 for YOLOv8n. Precision and recall follow the same trend, indicating that the lightweight model exhibits higher miss rates under challenging underwater conditions. The performance gap between the two models motivates the use of dynamic switching to balance accuracy and computational load.

\begin{table}[h]
\centering
\caption{Individual Model Performance on Test Dataset}
\label{tab:individual_performance}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@0.5} & \textbf{mAP@0.5:0.95} \\
\midrule
YOLOv8n & 0.499 & 0.397 & 0.265 & 0.145 \\
YOLOv8m & 0.738 & 0.604 & 0.455 & 0.273 \\
\bottomrule
\end{tabular}
\end{table}

Qualitative results from both models are shown in Figures~\ref{fig:yolov8n_detection} and~\ref{fig:yolov8m_detection}. YOLOv8n performs reliably in high-quality frames but fails to detect small or low-contrast objects in degraded scenes. YOLOv8m provides more stable predictions in low-visibility environments due to its larger capacity and deeper feature representations.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{8n.png}
\caption{Detection results produced by YOLOv8n under favorable underwater conditions.}
\label{fig:yolov8n_detection}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{8m.png}
\caption{Detection results produced by YOLOv8m under challenging visibility conditions.}
\label{fig:yolov8m_detection}
\end{figure}

Per-class performance is summarized in Table~\ref{tab:per_class_performance}. The medium-capacity model exhibits consistent improvements across all object categories. Improvements are most pronounced for small marine life (e.g., \texttt{animal\_crab}, \texttt{animal\_starfish}) and low-contrast debris types, confirming that higher model capacity is essential when image degradation reduces feature separability.

\begin{table}[h]
\centering
\caption{Per-Class mAP@0.5 Comparison}
\label{tab:per_class_performance}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Class} & \textbf{YOLOv8n} & \textbf{YOLOv8m} & \textbf{Improvement} \\
\midrule
rov & 0.806 & 0.932 & +12.6 pp \\
plant & 0.385 & 0.547 & +16.2 pp \\
animal\_fish & 0.402 & 0.549 & +14.7 pp \\
animal\_starfish & 0.440 & 0.844 & +40.4 pp \\
animal\_shells & 0.362 & 0.590 & +22.8 pp \\
animal\_crab & 0.086 & 0.432 & +34.6 pp \\
trash\_plastic & 0.570 & 0.794 & +22.4 pp \\
trash\_metal & 0.453 & 0.690 & +23.7 pp \\
trash\_paper & 0.320 & 0.603 & +28.3 pp \\
trash\_wood & 0.614 & 0.715 & +10.1 pp \\
\midrule
\textbf{Average} & \textbf{0.444} & \textbf{0.670} & \textbf{+22.6 pp} \\
\bottomrule
\end{tabular}
\end{table}

Detection counts across representative classes are shown in Table~\ref{tab:detection_counts}. The adaptive system consistently falls between YOLOv8n and YOLOv8m, reflecting selective use of the higher-capacity model in frames where the lightweight model underperforms. For difficult categories such as \texttt{animal\_etc} and \texttt{trash\_fishing\_gear}, the adaptive system recovers a substantial portion of the missed detections.

\begin{table}[h]
\centering
\caption{Detection Counts Across Representative Classes}
\label{tab:detection_counts}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Class} & \textbf{Adaptive} & \textbf{YOLOv8n} & \textbf{YOLOv8m} \\
\midrule
rov & 740 & 727 & 780 \\
plant & 62 & 53 & 109 \\
animal\_fish & 85 & 54 & 98 \\
animal\_starfish & 144 & 138 & 163 \\
animal\_shells & 22 & 21 & 87 \\
animal\_crab & 261 & 289 & 82 \\
animal\_eel & 83 & 63 & 99 \\
animal\_etc & 33 & 9 & 52 \\
trash\_etc & 362 & 284 & 425 \\
trash\_fabric & 66 & 56 & 73 \\
trash\_fishing\_gear & 35 & 11 & 108 \\
trash\_metal & 220 & 232 & 222 \\
trash\_paper & 14 & 15 & 27 \\
trash\_plastic & 357 & 370 & 344 \\
trash\_rubber & 13 & 12 & 12 \\
trash\_wood & 47 & 50 & 55 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:switching_performance} summarizes performance of the dynamic switching system. The approach achieves an mAP@0.5 of 0.393, corresponding to 86.4\% of YOLOv8m's accuracy while using YOLOv8n for 88.7\% of all frames. Average inference speed improves by a factor of 2.1 compared to a YOLOv8m-only baseline, and estimated energy consumption is reduced by 47.3\%.

\begin{table}[h]
\centering
\caption{Dynamic Switching System Performance}
\label{tab:switching_performance}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Overall mAP@0.5 & 0.393 \\
Precision & 0.641 \\
Recall & 0.522 \\
YOLOv8n usage & 88.7\% \\
YOLOv8m usage & 11.3\% \\
Total switches & 53 (5000 frames) \\
Average switch latency & 8.7 ms \\
\midrule
Accuracy retention vs YOLOv8m & 86.4\% \\
Speed improvement vs YOLOv8m & 2.1× \\
Energy savings vs YOLOv8m & 47.3\% \\
\bottomrule
\end{tabular}
\end{table}

Environmental breakdown results are shown in Table~\ref{tab:sequence_results}. Higher-turbidity sequences trigger more frequent activation of YOLOv8m, confirming that the quality score effectively captures scene difficulty.

\begin{table}[h]
\centering
\caption{Performance Across Environmental Conditions}
\label{tab:sequence_results}
\begin{tabular}{lccc}
\toprule
\textbf{Sequence} & \textbf{Avg Quality} & \textbf{YOLOv8m \%} & \textbf{mAP@0.5} \\
\midrule
A (Clear) & 0.64 & 5.8\% & 0.421 \\
B (Moderate) & 0.43 & 11.2\% & 0.387 \\
C (Low light) & 0.35 & 17.4\% & 0.368 \\
D (High turbidity) & 0.29 & 24.6\% & 0.352 \\
E (Variable) & 0.47 & 12.3\% & 0.391 \\
\midrule
\textbf{Weighted average} & 0.44 & 14.3\% & 0.384 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:quality_correlation} shows mean quality scores for frames processed by each model. Blur exhibits the highest separation, confirming its strong influence on switching decisions.

\begin{table}[h]
\centering
\caption{Quality Component Statistics by Selected Model}
\label{tab:quality_correlation}
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{YOLOv8n} & \textbf{YOLOv8m} \\
\midrule
Blur score & 0.612 & 0.287 \\
Brightness score & 0.701 & 0.534 \\
Contrast score & 0.628 & 0.398 \\
Overall quality & 0.647 & 0.406 \\
\bottomrule
\end{tabular}
\end{table}

\section{Discussion}

The results demonstrate that dynamic model switching provides a substantial improvement in efficiency while preserving most of the detection accuracy of the higher-capacity model. The system maintains 86.4\% of YOLOv8m’s mAP while achieving a 2.1× increase in inference speed and an estimated 47.3\% reduction in energy consumption. These gains indicate that adaptive computation is a practical alternative to deploying a single static detector on resource-constrained AUV platforms.

The weighting of blur, brightness, and contrast confirms that blur is the dominant predictor of frame difficulty in underwater imagery. Lower blur scores strongly correlate with conditions in which the lightweight model underperforms, and the switching mechanism consistently responds to these conditions. The hysteresis structure prevents oscillation around borderline quality values, and the cooldown interval ensures temporal stability during rapid scene changes.

Per-class analysis shows the largest accuracy improvements for small or low-contrast objects, such as \texttt{animal\_crab}, \texttt{animal\_starfish}, and several debris categories. These objects exhibit pronounced degradation under turbidity and low light, and the medium-capacity model provides the feature depth required to recover them. Safety-critical classes such as ROV components also benefit, reducing the likelihood of misclassification during navigation.

Energy efficiency results indicate that the dynamic system is suitable for long-duration AUV missions. Reducing computational load extends operational autonomy, enabling larger coverage areas without additional hardware resources. Since underwater conditions change across geography, depth, and time, the ability to adjust computational effort dynamically allows the system to maintain reliable detection without operator intervention.

Adaptive inference frameworks offer advantages over pruning, quantization, and knowledge distillation. Such compression techniques reduce model size but provide fixed capacity and often incur accuracy losses. Cascade architectures and early-exit networks introduce architectural dependencies and specialized training requirements. In contrast, the switching framework operates with two fully trained, independent models and does not impose modifications to detector structure or training procedures.

Limitations are observed in cases where image quality metrics fail to capture object-level difficulty. A small subset of frames demonstrates missed opportunities for switching when important objects appear in otherwise high-quality scenes. The cooldown interval also introduces brief delays during transitions, although these delays represent a minor fraction of total frames.

The approach is applicable beyond underwater waste detection. Domains characterized by fluctuating input quality, such as autonomous driving, medical imaging, aerial surveillance, and industrial inspection, can benefit from similar adaptive strategies. Any environment with variability in visibility, occlusion, noise, or illumination can leverage this mechanism to balance accuracy and computational demand in real time.

\section{Conclusion and Future Work}

A dynamic model switching framework for underwater waste detection is introduced, enabling real-time adjustment of computational capacity based on image quality. The method combines a lightweight and a medium-capacity YOLOv8 model through a hysteresis-driven selection mechanism supported by blur-, brightness-, and contrast-based quality assessment. The system maintains 86.4\% of the higher-capacity model’s accuracy while achieving a 2.1× increase in inference speed and a 47.3\% reduction in estimated energy consumption, establishing a practical balance between detection performance and resource efficiency for AUV deployment.

The approach improves detection consistency across degraded conditions, particularly for small and low-contrast objects, and stabilizes inference through controlled switching. The use of independent, fully trained models avoids architectural modifications and preserves compatibility with existing detection pipelines. Efficiency gains directly support longer operational autonomy and broader coverage in variable underwater environments.

Future developments focus on quality-aware refinement strategies, multi-level model hierarchies, learned quality predictors, temporal consistency mechanisms, embedded-hardware optimization, uncertainty-guided switching, and extensions to multi-task perception. These directions enable broader applicability to domains where input conditions fluctuate and computational resources remain constrained.

\end{document}