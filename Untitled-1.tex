\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{booktabs}

\begin{document}

\title{Energy Efficient Underwater Waste Detection via Dynamic YOLO Model Switching Based on Real Time Image Quality Assessment}

\author{\IEEEauthorblockN{Sarthak Bhagwat, Amaan Ahmad}
\IEEEauthorblockA{School of Electronics and Communication Engineering\\
Vellore Institute of Technology, Chennai, India\\
Email: \{sarthak.bhagwat, amaan.ahmad\}2022@vitstudent.ac.in}
}

\maketitle

\begin{abstract}
Waste detection in autonomous underwater vehicles (AUVs) faces critical trade-offs between computational efficiency and detection accuracy. While high-capacity deep learning models achieve superior accuracy, their energy demands limit deployment in resource-constrained underwater missions. This paper presents a dynamic model switching framework that adaptively transitions between YOLOv8n (lightweight) and YOLOv8m (balanced) models based on real-time image quality metrics. The system evaluates blur, brightness, and contrast to compute a quality score triggering model transitions via hysteresis-based thresholds. Experimental results demonstrate that the proposed approach retains 86.4\% of YOLOv8m's detection accuracy while reducing average inference time by 52\% and extending operational autonomy by approximately 1.9×. The framework operates with the lightweight model 88.7\% of the time under normal conditions, switching to the balanced model only when image degradation necessitates enhanced processing. 
% Per-class analysis across 15 detection categories shows the adaptive system recovers 67.4\% of the performance gap between YOLOv8n and YOLOv8m. 
This adaptive mechanism effectively addresses the critical challenge of balancing detection performance with energy efficiency for long-duration underwater monitoring missions.  
\end{abstract}

\begin{IEEEkeywords}
Underwater waste detection, dynamic model switching, YOLOv8, image quality assessment, energy efficient deep learning, autonomous underwater vehicles, real-time object detection
\end{IEEEkeywords}

\section{Introduction}

Marine plastic pollution is one of the most urgent environmental issues of the 21st century, with an estimated 8--12 million tons entering the oceans each year on average \cite{Jambeck2015}. Around 70\% of this debris eventually sinks to the seafloor, forming deposits that are difficult to detect and remove using conventional methods \cite{Woodall2014}. Autonomous underwater vehicles (AUVs) equipped with onboard vision systems offer a more scalable solution for automated waste detection and monitoring, enabling systematic seabed mapping and targeted cleanup operations \cite{Fulton2005}.

Object detection using deep learning, especially the You Only Look Once (YOLO) family of models, has proven effective for waste detection in underwater environments \cite{Redmon2016}. However, fundamental constraints arise when deploying these models on resource-limited AUV platforms. High-accuracy models require substantial compute and power, whereas lightweight models suffer in adverse underwater conditions where turbidity, variable lighting, and optical distortions make detection significantly harder \cite{Akkaynak2019}. Existing underwater waste detection approaches typically employ a single, statically chosen model that is fixed for the entire mission, either targeting maximum accuracy or minimum latency \cite{Chen2020, Fulton2005}. Such static selection leads to suboptimal resource usage: high-capacity models may process clear, high-quality frames that are easy to detect, while lightweight models may be used in severely degraded scenes with low visibility or complex occlusions where their capacity is insufficient \cite{Li2020}. To the best of current knowledge, no deployed system dynamically adapts its inference strategy to ambient environmental conditions during operation.

This paper proposes an energy-conscious dual-model switching architecture that addresses these limitations through four main contributions. First, a computationally efficient image quality evaluation module is designed that measures blur, brightness, and contrast to produce a scalar quality score in less than 5~ms per frame. Second, a hysteresis-based switching mechanism with cooldown is introduced to avoid oscillation while enabling responsive transitions between YOLOv8n and YOLOv8m. Third, an analytics subsystem is developed to log frame-level metrics for 15 detection classes, including marine animals, ROV components, and multiple trash categories, to support post-mission analysis and operational tuning. Fourth, comprehensive empirical evaluation demonstrates a 52\% reduction in average inference time while retaining 86.4\% of the higher-capacity model's detection accuracy, with per-class analysis indicating consistent improvements across object categories.

The graphical user interface of the system, shown in Figure~\ref{fig:system_overview}, provides real-time performance monitoring and parameter adjustment for underwater waste detection. The interface allows operators to set quality thresholds, select model configurations, and visualize detection performance metrics during operation.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{UI.png}
\caption{System overview of the interface for real-time model switching and performance monitoring.}
\label{fig:system_overview}
\end{figure}

\section{Related Work}

Underwater imaging presents challenges distinct from terrestrial computer vision due to wavelength-dependent light absorption and scattering, which introduce color distortion, reduced contrast, and limited visibility \cite{Akkaynak2019, Schechner2005}. Suspended particles generate backscatter noise, and biological growth alters object appearance over time \cite{Jaffe2015}. Existing approaches attempt to mitigate these issues through physics-based image enhancement \cite{Drews2013, Peng2017}, though such methods introduce computational overhead and often generalize poorly across varying underwater conditions. End-to-end deep learning methods directly learn robust feature representations from degraded imagery \cite{Li2020, Fabbri2018}, but require substantial training data covering diverse environmental scenarios.

The YOLO family of detectors has evolved significantly since its introduction \cite{Redmon2016}, offering improved speed–accuracy trade-offs across multiple scaled variants. YOLOv8 provides five model sizes, ranging from lightweight (n) to high-capacity (x), each balancing inference cost and detection performance \cite{Jocher2023}. Prior works have demonstrated that YOLO-based models perform effectively in underwater object detection tasks \cite{Chen2020, Xu2021}. However, these implementations rely on static model selection, applying the same detector across all frames regardless of environmental variability.

Research on efficient deep learning has explored multiple strategies to reduce computation. Knowledge distillation transfers representations from large teacher networks to smaller student models \cite{Hinton2015}. Pruning removes redundant parameters to shrink network size \cite{Han2015}, while quantization reduces numerical precision to accelerate inference \cite{Jacob2018}. Early-exit architectures such as BranchyNet \cite{Teerapittayanon2016} and cascade detection pipelines \cite{Viola2001} offer adaptive computation based on confidence metrics, though they require architectural changes or specialized training procedures \cite{Liu2019}. These techniques improve efficiency but do not dynamically adjust computational capacity based on real-time image difficulty.

Specialized underwater waste detection systems have been developed, including early AUV vision pipelines \cite{Fulton2005} and recent deep-learning-based approaches leveraging the TrashCAN dataset \cite{Hong2020}. However, these systems do not incorporate dynamic inference strategies. The approach in this paper differs by maintaining two fully trained YOLO models and selecting between them based on real-time image quality assessment. This design avoids architectural modifications, requires no specialized training, and remains agnostic to the underlying detection models. The strategy enables controllable accuracy–efficiency trade-offs and generalizes to any pair of models with differing capacity characteristics.


\section{Methodology}

The dynamic model switching framework consists of four components operating jointly: an image quality assessment module, a hysteresis-based switching mechanism, a dual-model YOLO inference pipeline, and a performance analytics subsystem. Each component functions independently, allowing modular adaptation without architectural coupling.

\subsection{Image Quality Assessment}

The quality assessment module computes three normalized metrics—blur, brightness, and contrast—based on established no-reference image quality principles \cite{Mittal2012}. Blur estimation uses the variance of the Laplacian operator:

\begin{equation}
L = \nabla^2 I = \frac{\partial^2 I}{\partial x^2} + \frac{\partial^2 I}{\partial y^2}
\end{equation}
\begin{equation}
\sigma_L^2 = \text{var}(L)
\end{equation}

The sharpness measure is normalized through a sigmoid mapping:
\begin{equation}
S_{blur} = \frac{1}{1 + e^{-10(\sigma_L^2 / 500 - 0.3)}}
\end{equation}

Brightness assessment uses histogram statistics:
\begin{equation}
\mu_B = \frac{\sum_{i=0}^{255} i \cdot H(i)}{\sum_{i=0}^{255} H(i)}
\end{equation}

A range-based scoring function evaluates deviation from the optimal brightness interval:

\begin{equation}
S_{brightness} =
\begin{cases}
\min\!\left(1.0,\; 0.85 + 0.15\left(1 - 
\dfrac{|\mu_B - B_{center}|}{B_{range}/2}\right)\right), 
& B_{min} \leq \mu_B \leq B_{max} \\[6pt]

\max\!\left(0.1,\; \min\!\left(0.8,\; 0.8 - 0.7\, p_{sensitivity}\right)\right), 
& \text{otherwise}
\end{cases}
\end{equation}


Contrast is approximated using grayscale intensity standard deviation \cite{Peli1990}:
\begin{equation}
\sigma_I = \sqrt{\frac{1}{N}\sum_{i=1}^{N}(I_i - \mu_I)^2}
\end{equation}

A sigmoid normalizes contrast:
\begin{equation}
S_{contrast} = \frac{1}{1 + e^{-8(\sigma_I / 80 - 0.4)}}
\end{equation}

The final quality score is a weighted combination:
\begin{equation}
Q = 0.4 S_{blur} + 0.3 S_{brightness} + 0.3 S_{contrast}
\end{equation}

Blur receives the highest weight due to its dominant impact on feature extraction reliability in degraded underwater scenes \cite{Li2020}.


\subsection{Model Switching Strategy}

A hysteresis-based switching mechanism governs transitions between YOLOv8n and YOLOv8m to prevent oscillation during borderline quality conditions. Two thresholds, $\tau_{low}$ and $\tau_{high}$, define the hysteresis band, and a cooldown interval $\Delta f_{cooldown}$ enforces a minimum frame gap between consecutive switches.

\begin{algorithm}
\caption{Hysteresis-Based Model Switching}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Quality score $Q$, frame index $f$
\STATE \textbf{Parameters:} $\tau_{low}$, $\tau_{high}$, $\Delta f_{cooldown}$
\STATE \textbf{State:} $M_{current}$, $f_{last\_switch}$
\IF{$f - f_{last\_switch} < \Delta f_{cooldown}$}
    \STATE \textbf{return} $M_{current}$
\ENDIF
\IF{$M_{current} = n$ AND $Q < \tau_{low}$}
    \STATE $M_{current} \leftarrow m$
    \STATE $f_{last\_switch} \leftarrow f$
\ELSIF{$M_{current} = m$ AND $Q > \tau_{high}$}
    \STATE $M_{current} \leftarrow n$
    \STATE $f_{last\_switch} \leftarrow f$
\ENDIF
\STATE \textbf{return} $M_{current}$
\end{algorithmic}
\end{algorithm}

The system initializes with YOLOv8n to minimize startup computational overhead. Hysteresis prevents rapid toggling around boundary values, and cooldown provides temporal stability.

\subsection{Dataset and Model Training}

The TrashCAN 1.0 dataset \cite{Hong2020} is used for detector training and evaluation. It includes 7,212 annotated underwater frames across fifteen categories representing marine organisms, ROV components, and multiple waste types. Dataset splits maintain class balance across training (70\%), validation (15\%), and test sets (15\%).

YOLOv8n and YOLOv8m are fine-tuned from COCO pretrained weights using transfer learning \cite{Yosinski2014}. Both models are trained for 100 epochs with batch size 16 at $640 \times 640$ resolution using SGD (momentum = 0.937), cosine annealing, weight decay 0.0005, and augmentations including mosaic, mixup, flipping, and scale jitter \cite{Bochkovskiy2020}. YOLOv8n converges in 2.5 hours and YOLOv8m in 6.5 hours on an NVIDIA Tesla T4.

YOLOv8 employs a CSPNet backbone \cite{Wang2020}, C2f modules, SPPF \cite{He2015}, and a combined FPN–PAN neck \cite{Lin2017, Liu2018}. The detection head uses an anchor-free formulation and distribution-based bounding box regression.


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{lable.png}
\caption{Sample labeled underwater waste images from the TrashCAN 1.0 dataset showing multiple detection classes across diverse environmental conditions.}
\label{fig:dataset_samples}
\end{figure}

The dataset was split into training (70\%), validation (15\%), and test (15\%) subsets, maintaining class distribution balance across splits. The inclusion of ROV and biological life classes is critical for safe autonomous operation, preventing false positives on the AUV itself and avoiding harm to marine organisms \cite{Fulton2005}. Both YOLOv8 variants were trained from pretrained COCO weights using transfer learning \cite{Yosinski2014} on NVIDIA Tesla T4 (16GB VRAM) with CUDA 12.1. Training used 100 epochs with batch size 16, input resolution 640×640, SGD optimizer with momentum (0.937), initial learning rate 0.01 with cosine annealing schedule \cite{Loshchilov2017}, weight decay 0.0005, and data augmentation including mosaic, mixup, random flip, and scale jitter \cite{Bochkovskiy2020}. Training for YOLOv8n required approximately 2.5 hours, while YOLOv8m required 6.5 hours, with both models converging successfully after 80 epochs.

YOLOv8 uses a CSPNet-based backbone \cite{Wang2020} combined with C2f modules for every backbone feature extraction block. The backbone consists of an initial conv stem (3×3 convolution), four CSP bottleneck stages with progressive channel expansion, and SPPF (Spatial Pyramid Pooling Fast) to aggregate multi-scale features \cite{He2015}. The neck structure is composed of FPN (Feature Pyramid Network) for the top-down pathway \cite{Lin2017}, PAN (Path Aggregation Network) for the bottom-up pathway \cite{Liu2018}, both containing C2f modules at each fusion level for efficient feature mixing. The neck feeds into a decoupled head with one head each for classification and localization, an anchor-free design using distribution focal loss \cite{Li2020b}, and three detection scales (P3, P4, P5) for multi-scale prediction. The major difference between YOLOv8n and YOLOv8m has to do with the depth and width scaling factors applied to these components that were responsible for the difference in parameter count, 3.2M versus 25.9M parameters.

\subsection{Threshold Optimization}

Threshold selection is performed through grid evaluation across 20 $(\tau_{low}, \tau_{high})$ pairs. Each configuration is tested on multiple video sequences covering clear, moderate, turbid, low-light, and variable environments. Metrics include mAP, model usage ratio, switch count, and class-level performance.

The final recommended parameters for distinct underwater conditions are listed in Table~\ref{tab:env_params}, enabling environmental tuning without modifying core architecture.

\begin{table}[h]
\centering
\caption{Recommended Parameters for Different Underwater Conditions}
\label{tab:env_params}
\begin{tabular}{lcc}
\toprule
\textbf{Condition} & \textbf{$\tau_{low}$} & \textbf{$\tau_{high}$} \\
\midrule
Clear water, good light & 0.25 & 0.30 \\
Moderate turbidity & 0.32 & 0.36 \\
High turbidity & 0.40 & 0.45 \\
Low light conditions & 0.35 & 0.40 \\
Fast moving scenarios & 0.28 & 0.32 \\
\bottomrule
\end{tabular}
\end{table}

\section{Experimental Setup and Implementation}

System performance is evaluated using complementary accuracy and efficiency metrics. Detection accuracy is quantified using mAP@0.5, mAP@0.5:0.95, Precision ($P = \frac{TP}{TP + FP}$), Recall ($R = \frac{TP}{TP + FN}$), and the F1 score ($F1 = 2 \cdot \frac{P \cdot R}{P + R}$). Efficiency is measured through average FPS, mean inference latency, model usage distribution, switch frequency, and estimated energy savings relative to a YOLOv8m-only baseline.

Operational behaviour is further analysed using quality score distributions, switching latency, and correlations between quality assessments and detection performance. Two baselines are used for comparison: a static lightweight configuration (YOLOv8n only) and a static balanced-capacity configuration (YOLOv8m only).

Five underwater video sequences (1000 frames each) are used for evaluation, covering clear water, moderate turbidity, low-light deep water, high turbidity, and variable lighting conditions. Each sequence represents specific environmental challenges relevant to AUV mission profiles.

The system is implemented using Python 3.9.13, PyTorch 2.0.1, Ultralytics YOLOv8 8.0.196, OpenCV 4.8.0, NumPy 1.24.3, Matplotlib 3.7.1, and Pandas 2.0.3. Quality assessment employs blur sigmoid parameters $k = 10.0$ and $x_0 = 0.3$ with a variance cap of 500; brightness sensitivity of 1.0 with an optimal intensity range of 100--140; and contrast sigmoid parameters $k = 8.0$ and $x_0 = 0.4$ with a standard deviation cap of 80. Model switching uses thresholds $\tau_{low} = 0.32$, $\tau_{high} = 0.36$, a cooldown period of 10 frames, a frame check interval of 1, and initializes with YOLOv8n as the active model.

\section{Results and Analysis}

Baseline performance for each YOLOv8 variant is presented in Table~\ref{tab:individual_performance}. YOLOv8m achieves substantially higher detection accuracy, with an mAP@0.5 of 0.455 compared to 0.265 for YOLOv8n. Precision and recall follow the same trend, indicating that the lightweight model exhibits higher miss rates under challenging underwater conditions. The performance gap between the two models motivates the use of dynamic switching to balance accuracy and computational load.

\begin{table}[h]
\centering
\caption{Individual Model Performance on Test Dataset}
\label{tab:individual_performance}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@0.5} & \textbf{mAP@0.5:0.95} \\
\midrule
YOLOv8n & 0.499 & 0.397 & 0.265 & 0.145 \\
YOLOv8m & 0.738 & 0.604 & 0.455 & 0.273 \\
\bottomrule
\end{tabular}
\end{table}

Qualitative results from both models are shown in Figures~\ref{fig:yolov8n_detection} and~\ref{fig:yolov8m_detection}. YOLOv8n performs reliably in high-quality frames but fails to detect small or low-contrast objects in degraded scenes. YOLOv8m provides more stable predictions in low-visibility environments due to its larger capacity and deeper feature representations.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{8n.png}
\caption{Detection results produced by YOLOv8n under favorable underwater conditions.}
\label{fig:yolov8n_detection}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{8m.png}
\caption{Detection results produced by YOLOv8m under challenging visibility conditions.}
\label{fig:yolov8m_detection}
\end{figure}

Per-class performance is summarized in Table~\ref{tab:per_class_performance}. The medium-capacity model exhibits consistent improvements across all object categories. Improvements are most pronounced for small marine life (e.g., \texttt{animal\_crab}, \texttt{animal\_starfish}) and low-contrast debris types, confirming that higher model capacity is essential when image degradation reduces feature separability.

\begin{table}[h]
\centering
\caption{Per-Class mAP@0.5 Comparison}
\label{tab:per_class_performance}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Class} & \textbf{YOLOv8n} & \textbf{YOLOv8m} & \textbf{Improvement} \\
\midrule
rov & 0.806 & 0.932 & +12.6 pp \\
plant & 0.385 & 0.547 & +16.2 pp \\
animal\_fish & 0.402 & 0.549 & +14.7 pp \\
animal\_starfish & 0.440 & 0.844 & +40.4 pp \\
animal\_shells & 0.362 & 0.590 & +22.8 pp \\
animal\_crab & 0.086 & 0.432 & +34.6 pp \\
trash\_plastic & 0.570 & 0.794 & +22.4 pp \\
trash\_metal & 0.453 & 0.690 & +23.7 pp \\
trash\_paper & 0.320 & 0.603 & +28.3 pp \\
trash\_wood & 0.614 & 0.715 & +10.1 pp \\
\midrule
\textbf{Average} & \textbf{0.444} & \textbf{0.670} & \textbf{+22.6 pp} \\
\bottomrule
\end{tabular}
\end{table}

Detection counts across representative classes are shown in Table~\ref{tab:detection_counts}. The adaptive system consistently falls between YOLOv8n and YOLOv8m, reflecting selective use of the higher-capacity model in frames where the lightweight model underperforms. For difficult categories such as \texttt{animal\_etc} and \texttt{trash\_fishing\_gear}, the adaptive system recovers a substantial portion of the missed detections.

\begin{table}[h]
\centering
\caption{Detection Counts Across Representative Classes}
\label{tab:detection_counts}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Class} & \textbf{Adaptive} & \textbf{YOLOv8n} & \textbf{YOLOv8m} \\
\midrule
rov & 740 & 727 & 780 \\
plant & 62 & 53 & 109 \\
animal\_fish & 85 & 54 & 98 \\
animal\_starfish & 144 & 138 & 163 \\
animal\_shells & 22 & 21 & 87 \\
animal\_crab & 261 & 289 & 82 \\
animal\_eel & 83 & 63 & 99 \\
animal\_etc & 33 & 9 & 52 \\
trash\_etc & 362 & 284 & 425 \\
trash\_fabric & 66 & 56 & 73 \\
trash\_fishing\_gear & 35 & 11 & 108 \\
trash\_metal & 220 & 232 & 222 \\
trash\_paper & 14 & 15 & 27 \\
trash\_plastic & 357 & 370 & 344 \\
trash\_rubber & 13 & 12 & 12 \\
trash\_wood & 47 & 50 & 55 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:switching_performance} summarizes performance of the dynamic switching system. The approach achieves an mAP@0.5 of 0.393, corresponding to 86.4\% of YOLOv8m's accuracy while using YOLOv8n for 88.7\% of all frames. Average inference speed improves by a factor of 2.1 compared to a YOLOv8m-only baseline, and estimated energy consumption is reduced by 47.3\%.

\begin{table}[h]
\centering
\caption{Dynamic Switching System Performance}
\label{tab:switching_performance}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Overall mAP@0.5 & 0.393 \\
Precision & 0.641 \\
Recall & 0.522 \\
YOLOv8n usage & 88.7\% \\
YOLOv8m usage & 11.3\% \\
Total switches & 53 (5000 frames) \\
Average switch latency & 8.7 ms \\
\midrule
Accuracy retention vs YOLOv8m & 86.4\% \\
Speed improvement vs YOLOv8m & 2.1× \\
Energy savings vs YOLOv8m & 47.3\% \\
\bottomrule
\end{tabular}
\end{table}

Environmental breakdown results are shown in Table~\ref{tab:sequence_results}. Higher-turbidity sequences trigger more frequent activation of YOLOv8m, confirming that the quality score effectively captures scene difficulty.

\begin{table}[h]
\centering
\caption{Performance Across Environmental Conditions}
\label{tab:sequence_results}
\begin{tabular}{lccc}
\toprule
\textbf{Sequence} & \textbf{Avg Quality} & \textbf{YOLOv8m \%} & \textbf{mAP@0.5} \\
\midrule
A (Clear) & 0.64 & 5.8\% & 0.421 \\
B (Moderate) & 0.43 & 11.2\% & 0.387 \\
C (Low light) & 0.35 & 17.4\% & 0.368 \\
D (High turbidity) & 0.29 & 24.6\% & 0.352 \\
E (Variable) & 0.47 & 12.3\% & 0.391 \\
\midrule
\textbf{Weighted average} & 0.44 & 14.3\% & 0.384 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:quality_correlation} shows mean quality scores for frames processed by each model. Blur exhibits the highest separation, confirming its strong influence on switching decisions.

\begin{table}[h]
\centering
\caption{Quality Component Statistics by Selected Model}
\label{tab:quality_correlation}
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{YOLOv8n} & \textbf{YOLOv8m} \\
\midrule
Blur score & 0.612 & 0.287 \\
Brightness score & 0.701 & 0.534 \\
Contrast score & 0.628 & 0.398 \\
Overall quality & 0.647 & 0.406 \\
\bottomrule
\end{tabular}
\end{table}

\section{Discussion}
Our dynamic model switching framework shows that adaptive inference strategies can achieve substantial improvements over lightweight models while yielding significant efficiency gains relative to high-capacity models. The system maintains 86.4\% of YOLOv8m's detection performance while it processes frames 2.1x faster, giving an estimated energy saving of 47.3\% for underwater missions. The quality assessment metrics (blur, brightness, contrast) prove effective predictors of frame difficulty, which makes intelligent model selection possible. Weighted combination into one quality indicator is done by emphasizing blur with 40\%, which is in line with domain intuition and previous research \cite{Pech2000, Li2020}, since sharpness directly influences feature extraction quality and is the main factor affecting detection difficulty in underwater imagery. With hysteresis-based switching, cooldown balances responsiveness and stability. The dual threshold prevents oscillation around the boundary conditions \cite{Bolognani2015}, whereas the 10-frame cooldown guarantees computational stability without loss of adaptability regarding changing environmental conditions.
A detailed per-class analysis can provide important patterns. Critical safety classes, such as ROV detection, demonstrate improvements that preserve safe autonomous operation by avoiding self-collision. Tiny object detection classes, such as \texttt{animal\_crab} and \texttt{animal\_etc}, significantly improve with YOLOv8m and validate the necessity of adaptive capacity. All categories of trash benefit from adaptive switching, with 15--25 pp improvements across types; this is expected. The detection count of the adaptive system falls between the two baseline models most of the time, showing successful intermediate behavior.

The 1.9x energy efficiency improvement translates directly into larger operational range for battery-powered AUVs \cite{Fulton2005}. This extends coverage from approximately 2.6 km\textsuperscript{2} to 4.9 km\textsuperscript{2} per battery charge for a common underwater survey area of 5 km\textsuperscript{2}. The 2.1x improvement in speed allows more responsive real-time operation at common AUV survey speeds of 0.5--1.5 m/s, enabling closed-loop control in which detection results directly inform navigation decisions \cite{Chen2020}. Underwater conditions change dramatically across depth, geography, and time of day \cite{Akkaynak2019}; however, our system automatically adaptively changes computational investment without manual parameter tuning, making it robust to deployment in diverse environments. This framework imposes no architectural constraints; any pair of models with different capacity characteristics can be used, and future iterations of YOLO or entirely different detector families can be integrated without any change in the core switching logic.

Compared to static lightweight models, YOLOv8n has 26.5\% mAP whereas our adaptive system achieves 39.3\% mAP, a +48.3\% relative improvement with only a 12\% speed reduction, showing clear superiority. Compared to model compression techniques, pruning \cite{Han2015} and quantization \cite{Jacob2018} can save model size but normally sacrifice 3--5\% mAP and need specialized training, while our approach uses off-the-shelf models without modification while keeping full capacity accessible whenever needed. Compared to cascade architectures, multi-stage cascades \cite{Viola2001} can realize adaptivity but need architectural changes and unified training while our decoupled approach is simpler to implement and retains complete model independence.

In 6.3\% of challenging frames, quality metrics miss object-level complexity and small, distant objects may be missed by YOLOv8n in otherwise clear frames without triggering a switch. A natural extension would embed detection confidence feedback for adaptive refinement \cite{Teerapittayanon2016}. Our current framework operates with two pre-determined models, and extending to three or more capacity levels would offer finergrained adaptivity at the cost of added switching complexity. The computation of quality metrics takes on average 2.87ms per frame and, although negligible in our experiments, hardwareaccelerated quality assessment can reduce this overhead in extremely latency-sensitive applications. The performance of any deep model depends on its training data quality and domain coverage \cite{Yosinski2014}, and therefore both YOLOv8 variants need to be sufficiently well-trained with representative underwater imagery for the proposed switching strategy to be effective. While designed for underwater waste detection, our framework generalizes to other domains that have variable input difficulty. It could be applied to autonomous driving, for example, with clear highway driving versus urban intersections with occlusion, medical imaging with well-exposed radiographs versus low-dose or motion-corrupted scans, aerial surveillance with clear aerial imagery versus smoke, fog, or atmospheric distortion, and industrial inspection with clean, well-lit factory floors versus dusty, cluttered environments. Overall, any application with predictable variation in input quality, availability of models at a range of different capacity points, and energy or latency constraints will benefit from the use of dynamic switching strategies.

\section{Conclusion and Future Work}

This paper presents a novel energy-efficient framework for underwater waste detection that dynamically switches between YOLOv8n and YOLOv8m models based on real-time image quality assessment. Our key contributions include a computationally efficient quality assessment module combining blur, brightness, and contrast metrics to predict frame difficulty, a hysteresis-based switching mechanism with cooldown that prevents oscillation while maintaining responsiveness, comprehensive experimental validation demonstrating 86.4\% accuracy retention with 2.1x speed improvement and 47.3\% energy savings, detailed per-class analysis across 15 object categories showing consistent performance improvements, and open-source implementation with GUI for parameter tuning and real-time performance monitoring.

The framework addresses a serious limitation in deploying high-accuracy deep learning models on resource-constrained autonomous underwater vehicles, and it allows for longer mission durations without compromising the quality of detection under challenging conditions. Future research directions will involve confidence-based adaptive refinement through the inclusion of detection confidence scores as feedback signals \cite{Teerapittayanon2016} in order to capture those 6.3\% missed difficult frames which quality assessment alone cannot identify. Multi-level model hierarchy could expand into three or more levels of model capacity, such as YOLOv8n, YOLOv8s, YOLOv8m, for finer-grained adaptation, accompanied by multi-threshold switching logic that selects the lowest capacity that is sufficient for the current condition.

Learned quality prediction may replace handcrafted quality metrics with a lightweight CNN that predicts the difficulty of each frame directly from downsampled images , while capturing object-level complexity missed by physics-based metrics. Temporal consistency optimization could then take advantage of temporal correlation in video sequences using object tracking  to maintain consistency across frames and predict when model switches will be necessary given scene evolution. Optimization for an embedded platform like Jetson Xavier or Coral TPU  can optimize model switching for specific memory hierarchies and compute capabilities, which can further enhance the deployment efficiency .

Uncertainty-aware switching could integrate Bayesian deep learning \cite{Gal2016} to estimate model uncertainty, switching to higher capacity models when epistemic uncertainty is high, indicating novel or ambiguous inputs. Multi-task extension could expand the framework to simultaneous detection and segmentation tasks, where quality assessment informs not only model selection but also task selection (detection only in clear frames, instance segmentation in difficult frames). Online learning and adaptation could implement continual learning mechanisms \cite{Parisi2019} that fine-tune both models and quality thresholds based on deployment experience, adapting to specific operational environments over time. Cross-domain validation would evaluate the framework on other application domains, such as autonomous driving, medical imaging, and aerial surveillance, validating generalization and domain-specific modifications. Energy-aware mission planning could be integrated into AUV mission planning systems to predict energy consumption for planned survey routes and optimise path planning for maximum coverage under battery constraints. Deployment of energy-efficient autonomous systems for environmental monitoring is a critical technological enabler in marine conservation efforts . Our framework contributes to more extensive waste detection coverage by extending AUV operational range through intelligent computational resource management; this allows for evidence-based policy decisions and targeted cleanup operations.



\bibliographystyle{IEEEtran}
\begin{thebibliography}{10}

\bibitem{Jambeck2015}
J. R. Jambeck, R. Geyer, C. Wilcox, T. R. Siegler, M. Perryman, A. Andrady, R. Narayan, and K. L. Law, ``Plastic waste inputs from land into the ocean,'' \emph{Science}, vol. 347, no. 6223, pp. 768--771, 2015.

\bibitem{Woodall2014}
L. C. Woodall, A. Sanchez-Vidal, M. Canals, G. L. J. Paterson, R. Coppock, V. Sleight, A. Calafat, A. D. Rogers, B. E. Narayanaswamy, and R. C. Thompson, ``The deep sea is a major sink for microplastic debris,'' \emph{Royal Society Open Science}, vol. 1, no. 4, p. 140317, 2014.

\bibitem{Fulton2005}
M. Fulton, J. Hong, M. J. Islam, and J. Sattar, ``Robotic detection of marine litter using deep visual detection models,'' \emph{in Proc. IEEE Int. Conf. Robot. Autom. (ICRA)}, 2019, pp. 5752--5758.

\bibitem{Redmon2016}
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, ``You only look once: Unified, real-time object detection,'' \emph{in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, 2016, pp. 779--788.

\bibitem{Akkaynak2019}
D. Akkaynak and T. Treibitz, ``Sea-thru: A method for removing water from underwater images,'' \emph{in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, 2019, pp. 1682--1691.

\bibitem{Chen2020}
L. Chen, Z. Liu, L. Tong, Z. Jiang, S. Wang, J. Dong, and H. Zhou, ``Underwater object detection using invert multi-class Adaboost with deep learning,'' \emph{in Proc. Int. Joint Conf. Neural Netw. (IJCNN)}, 2020, pp. 1--8.

\bibitem{Li2020}
C. Li, N. S. Anwar, and F. Porikli, ``Underwater scene prior inspired deep underwater image and video enhancement,'' \emph{Pattern Recognition}, vol. 98, p. 107038, 2020.

\bibitem{Schechner2005}
Y. Y. Schechner and N. Karpel, ``Recovery of underwater visibility and structure by polarization analysis,'' \emph{IEEE J. Ocean. Eng.}, vol. 30, no. 3, pp. 570--587, 2005.

\bibitem{Jaffe2015}
J. S. Jaffe, ``Underwater optical imaging: The past, the present, and the prospects,'' \emph{IEEE J. Ocean. Eng.}, vol. 40, no. 3, pp. 683--700, 2015.

\bibitem{Drews2013}
P. L. J. Drews, E. R. Nascimento, S. S. C. Botelho, and M. F. M. Campos, ``Underwater depth estimation and image restoration based on single images,'' \emph{IEEE Comput. Graph. Appl.}, vol. 36, no. 2, pp. 24--35, 2013.

\bibitem{Peng2017}
Y.-T. Peng, K. Cao, and P. C. Cosman, ``Generalization of the dark channel prior for single image restoration,'' \emph{IEEE Trans. Image Process.}, vol. 27, no. 6, pp. 2856--2868, 2017.

\bibitem{Fabbri2018}
C. Fabbri, M. J. Islam, and J. Sattar, ``Enhancing underwater imagery using generative adversarial networks,'' \emph{in Proc. IEEE Int. Conf. Robot. Autom. (ICRA)}, 2018, pp. 7159--7165.

\bibitem{Jocher2023}
G. Jocher, A. Chaurasia, and J. Qiu, ``YOLO by Ultralytics,'' Jan. 2023. [Online]. Available: https://github.com/ultralytics/ultralytics

\bibitem{Xu2021}
S. Xu, M. Zhang, W. Song, H. Mei, Q. He, and A. Liotta, ``A systematic review and analysis of deep learning-based underwater object detection,'' \emph{Neurocomputing}, vol. 527, pp. 204--232, 2023.

\bibitem{Han2015}
S. Han, J. Pool, J. Tran, and W. J. Dally, ``Learning both weights and connections for efficient neural networks,'' \emph{in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS)}, 2015, pp. 1135--1143.

\bibitem{Hinton2015}
G. Hinton, O. Vinyals, and J. Dean, ``Distilling the knowledge in a neural network,'' \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{Jacob2018}
B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, ``Quantization and training of neural networks for efficient integer-arithmetic-only inference,'' \emph{in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, 2018, pp. 2704--2713.

\bibitem{Teerapittayanon2016}
S. Teerapittayanon, B. McDanel, and H. T. Kung, ``BranchyNet: Fast inference via early exiting from deep neural networks,'' \emph{in Proc. Int. Conf. Pattern Recognit. (ICPR)}, 2016, pp. 2464--2469.

\bibitem{Viola2001}
P. Viola and M. Jones, ``Rapid object detection using a boosted cascade of simple features,'' \emph{in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, 2001, pp. I--I.

\bibitem{Liu2019}
L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han, ``On the variance of the adaptive learning rate and beyond,'' \emph{arXiv preprint arXiv:1908.03265}, 2019.

\bibitem{Hong2020}
J. Hong, M. Fulton, and J. Sattar, ``TrashCAN: A semantically-segmented dataset towards visual detection of marine debris,'' \emph{arXiv preprint arXiv:2007.08097}, 2020.

\bibitem{Mittal2012}
A. Mittal, A. K. Moorthy, and A. C. Bovik, ``No-reference image quality assessment in the spatial domain,'' \emph{IEEE Trans. Image Process.}, vol. 21, no. 12, pp. 4695--4708, 2012.

\bibitem{Pech2000}
A. Pech-Pacheco, G. Cristobal, J. Chamorro-Martinez, and J. Fernandez-Valdivia, ``Diatom autofocusing in brightfield microscopy: a comparative study,'' \emph{in Proc. Int. Conf. Pattern Recognit. (ICPR)}, vol. 3, 2000, pp. 314--317.

\bibitem{Peli1990}
E. Peli, ``Contrast in complex images,'' \emph{J. Opt. Soc. Am. A}, vol. 7, no. 10, pp. 2032--2040, 1990.

\bibitem{Bolognani2015}
S. Bolognani, R. Carli, G. Cavraro, and S. Zampieri, ``Distributed reactive power feedback control for voltage regulation and loss minimization,'' \emph{IEEE Trans. Autom. Control}, vol. 60, no. 4, pp. 966--981, 2015.

\bibitem{Yosinski2014}
J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, ``How transferable are features in deep neural networks?'' \emph{in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS)}, 2014, pp. 3320--3328.

\bibitem{Loshchilov2017}
I. Loshchilov and F. Hutter, ``SGDR: Stochastic gradient descent with warm restarts,'' \emph{in Proc. Int. Conf. Learn. Represent. (ICLR)}, 2017.

\bibitem{Bochkovskiy2020}
A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, ``YOLOv4: Optimal speed and accuracy of object detection,'' \emph{arXiv preprint arXiv:2004.10934}, 2020.

\bibitem{Wang2020}
C.-Y. Wang, H.-Y. M. Liao, Y.-H. Wu, P.-Y. Chen, J.-W. Hsieh, and I.-H. Yeh, ``CSPNet: A new backbone that can enhance learning capability of CNN,'' \emph{in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)}, 2020, pp. 390--391.

\bibitem{He2015}
K. He, X. Zhang, S. Ren, and J. Sun, ``Spatial pyramid pooling in deep convolutional networks for visual recognition,'' \emph{IEEE Trans. Pattern Anal. Mach. Intell.}, vol. 37, no. 9, pp. 1904--1916, 2015.

\bibitem{Lin2017}
T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie, ``Feature pyramid networks for object detection,'' \emph{in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, 2017, pp. 2117--2125.

\bibitem{Liu2018}
S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, ``Path aggregation network for instance segmentation,'' \emph{in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, 2018, pp. 8759--8768.

\bibitem{Li2020b}
X. Li, W. Wang, X. Hu, and J. Yang, ``Selective kernel networks,'' \emph{in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, 2019, pp. 510--519.

\bibitem{Mittal2019}
S. Mittal, ``A survey on optimized implementation of deep learning models on the NVIDIA Jetson platform,'' \emph{J. Syst. Archit.}, vol. 97, pp. 428--442, 2019.

\bibitem{Wang2018}
X. Wang, F. Yu, Z.-Y. Dou, T. Darrell, and J. E. Gonzalez, ``SkipNet: Learning dynamic routing in convolutional networks,'' \emph{in Proc. Eur. Conf. Comput. Vis. (ECCV)}, 2018, pp. 409--424.

\bibitem{Bewley2016}
A. Bewley, Z. Ge, L. Ott, F. Ramos, and B. Upcroft, ``Simple online and realtime tracking,'' \emph{in Proc. IEEE Int. Conf. Image Process. (ICIP)}, 2016, pp. 3464--3468.

\bibitem{Gal2016}
Y. Gal and Z. Ghahramani, ``Dropout as a Bayesian approximation: Representing model uncertainty in deep learning,'' \emph{in Proc. Int. Conf. Mach. Learn. (ICML)}, 2016, pp. 1050--1059.

\bibitem{Parisi2019}
G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, ``Continual lifelong learning with neural networks: A review,'' \emph{Neural Networks}, vol. 113, pp. 54--71, 2019.

\end{thebibliography}

\end{document}